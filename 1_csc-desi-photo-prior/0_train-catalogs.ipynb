{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n",
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n",
      "\n",
      "WARNING: version mismatch between CFITSIO header (v4.000999999999999) and linked library (v4.01).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import data_path\n",
    "from scripts.cross_match_scripts import pandas_to_fits\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterparts and field sources catalogs construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DESI-CSC 50 arcsec cone search match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing: load DESI-CSC 50 arcsec cone search match and join it with CSC data frame to get fluxes/errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "desi_csc_orig = pd.read_pickle(data_path+'/csc_desi_r50_gaia.gz_pkl',compression='gzip')\n",
    "desi_csc_orig['name_csc'] = desi_csc_orig['name_csc'].astype(str).str[2:-2] #fix bug in name_csc string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load CSC  raw data so that I get positional errors and X-ray fluxes.\n",
    "#it will be fixed in the future by MB #TODO\n",
    "csc_orig = pd.read_pickle(data_path+'/csc_init_df.gz_pkl',compression='gzip')\n",
    "csc_orig.rename(columns={'name':'name_csc'},inplace=True) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add chanda localisation error and flux\n",
    "desi_csc_orig = desi_csc_orig.merge(csc_orig[['name_csc', 'flux_csc_05_2', 'r_98_csc' ]], on='name_csc')\n",
    "\n",
    "\n",
    "#add magnitudes (NOT DEREDDENED) #TODO \n",
    "#later make cut on the SNR of flux #TODO\n",
    "@np.vectorize\n",
    "def flux2mag(flux):\n",
    "    if  flux<=0 or np.isnan(flux):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return 22.5 - 2.5 * np.log10(flux)\n",
    "\n",
    "\n",
    "for flux_name in ['flux_g', 'flux_r', 'flux_z',\n",
    "                    'flux_w1', 'flux_w2', 'flux_w3', 'flux_w4']:\n",
    "\n",
    "    desi_csc_orig['mag_' + flux_name.split('_')[1]] = flux2mag(desi_csc_orig[flux_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary filters and DESI duplicates (ra,dec) removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: filter brick ID\n",
    "#next sort by name_csc, ra, dec and flux_g, with flux_g from largest to smallest\n",
    "desi_csc = desi_csc_orig.query('brick_primary == True')\n",
    "desi_csc = desi_csc.sort_values(by=['name_csc', 'ra_csc', 'dec_csc', 'flux_g'], ascending=[True, True, True, False])\n",
    "\n",
    "#step 2: remove duplicates in ['ra_csc', 'dec_csc', 'ra', 'dec'] and keep the one with the highest flux_g \n",
    "desi_csc = desi_csc.drop_duplicates(subset=['ra_csc', 'dec_csc', 'ra', 'dec'], keep='first')\n",
    "\n",
    "\n",
    "#assign desi_id\n",
    "tm_desi_id=desi_csc['release'].astype(str)+'_'+desi_csc['brickid'].astype(str)+'_'+desi_csc['objid'].astype(str)\n",
    "desi_csc['desi_id'] = tm_desi_id\n",
    "\n",
    "#sort by sep_csc so that the closest objects are first\n",
    "desi_csc.sort_values(by=['name_csc', 'sep_csc'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating source densities and false association radius (r_false) for each CSC source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annuli_area_deg2(r_in_arcsec, r_out_arcsec):\n",
    "    r_in_deg = r_in_arcsec/3600\n",
    "    r_out_deg = r_out_arcsec/3600\n",
    "    area = np.pi*(r_out_deg**2 - r_in_deg**2)\n",
    "    return area\n",
    "\n",
    "def r_false(desi_rho_deg2, thresh = 0.03):\n",
    "    ''' Belvedersky+ 2022 '''\n",
    "    desi_rho_arcsec2 = desi_rho_deg2/(3600**2)\n",
    "    return np.sqrt(-np.log(1-thresh)/(np.pi*desi_rho_arcsec2))\n",
    "\n",
    "\n",
    "#make a dataframe of CSC sources\n",
    "csc = pd.DataFrame(desi_csc.groupby(by = 'name_csc', ).agg({'ra_csc': np.mean, 'dec_csc': np.mean}))\n",
    "\n",
    "\n",
    "#calculate total number of DESI sources withing 10-50 arcsec of each CSC source\n",
    "csc[['desi_10_50']] =  desi_csc.groupby(by = ['name_csc', pd.cut(desi_csc.sep_csc, [10, 50]) ]).size().unstack()\n",
    "\n",
    "\n",
    "#calculate source densities for 10-50'' annulus\n",
    "csc['src_dens_deg2_10_50'] = csc['desi_10_50']/annuli_area_deg2(10, 50)\n",
    "\n",
    "#drop sources with  zeros in the 10-50'' source density, 4 sources\n",
    "csc.drop(csc[csc.src_dens_deg2_10_50 == 0].index, inplace=True)\n",
    "\n",
    "#assign r_false for 10-50'' annulus density\n",
    "csc['r_false_003_dens_10_50'] = r_false(csc['src_dens_deg2_10_50'])\n",
    "csc.drop(columns=['desi_10_50', 'src_dens_deg2_10_50'], inplace=True)\n",
    "\n",
    "csc.reset_index(inplace=True)\n",
    "\n",
    "#assign r_false for each CSC source from csc dataframe to desi_csc dataframe\n",
    "desi_csc = desi_csc.merge(csc[['name_csc','r_false_003_dens_10_50']], on='name_csc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    77254\n",
      "2     5714\n",
      "3       81\n",
      "4       10\n",
      "5        1\n",
      "Name: sep_csc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#calculate for each CSC source the number of DESI sources within r_false\n",
    "tmp_df = pd.DataFrame(desi_csc.query('sep_csc <= r_false_003_dens_10_50').groupby(by = 'name_csc').sep_csc.apply('count'))\n",
    "print(tmp_df.sep_csc.value_counts())\n",
    "tmp_df.columns = ['sep_less_r_false']\n",
    "\n",
    "#merge with desi_csc dataframe\n",
    "desi_csc = desi_csc.merge(tmp_df, on='name_csc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterpart  assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "desi_csc_ctps = desi_csc.query('sep_csc <= r_false_003_dens_10_50 & sep_less_r_false==1 & sep_csc < r_98_csc ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete  22  duplicated desi_id\n",
      "Final number of counterparts: 77111\n"
     ]
    }
   ],
   "source": [
    "print('delete ', desi_csc_ctps.duplicated(subset='desi_id').sum(), ' duplicated desi_id')\n",
    "desi_csc_ctps = desi_csc_ctps.drop_duplicates(subset='desi_id')\n",
    "print('Final number of counterparts:', len(desi_csc_ctps))\n",
    "desi_csc_ctps['is_counterpart'] = True\n",
    "#dublicates will appear here if untreated\n",
    "#desi_csc_ctps.pivot_table(values='name_csc', index='desi_id', aggfunc='count').sort_values(by = 'name_csc',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field source assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible field sources:  4945706\n"
     ]
    }
   ],
   "source": [
    "desi_csc_field = desi_csc.query(\"sep_csc>=10 & sep_csc<=50 \")\n",
    "print('number of possible field sources: ', len(desi_csc_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of secure field sources  (0.1 of it):  326948\n"
     ]
    }
   ],
   "source": [
    "tmp_df = desi_csc_field.groupby(by = 'desi_id', ).agg({'name_csc': 'count'})\n",
    "clear_field_ids = tmp_df.index[tmp_df['name_csc']==1]\n",
    "desi_csc_field = desi_csc_field[desi_csc_field.desi_id.isin(clear_field_ids)==True]\n",
    "fraction_to_retain = 0.1\n",
    "desi_csc_field = desi_csc_field.sample(frac=fraction_to_retain)\n",
    "desi_csc_field['is_counterpart'] = False\n",
    "desi_csc_field['flux_csc_05_2'] = np.nan\n",
    "desi_csc_field['r_98_csc'] = np.nan\n",
    "\n",
    "print(f'Final number of secure field sources  ({fraction_to_retain} of it): ', len(desi_csc_field))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the combined catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sdbykov/work/lockman_hole/1_csc-desi-photo-prior/0_train-catalogs.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sdbykov/work/lockman_hole/1_csc-desi-photo-prior/0_train-catalogs.ipynb#ch0000020?line=0'>1</a>\u001b[0m desi_csc_training \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([desi_csc_ctps, desi_csc_field])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sdbykov/work/lockman_hole/1_csc-desi-photo-prior/0_train-catalogs.ipynb#ch0000020?line=1'>2</a>\u001b[0m \u001b[39m#save cat to pickle\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sdbykov/work/lockman_hole/1_csc-desi-photo-prior/0_train-catalogs.ipynb#ch0000020?line=2'>3</a>\u001b[0m desi_csc_training\u001b[39m.\u001b[39mto_pickle(data_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcsc_desi_r50_gaia_traning.gz_pkl\u001b[39m\u001b[39m'\u001b[39m, compression\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "desi_csc_training = pd.concat([desi_csc_ctps, desi_csc_field])\n",
    "#save cat to pickle\n",
    "desi_csc_training.to_pickle(data_path+'csc_desi_r50_gaia_traning.gz_pkl', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv_hea')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1dc9197fbdaf2df20a0ea77561d79844f791293e1aa1b8fac12d88bf49496cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
